{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T20:46:31.139212Z",
     "start_time": "2024-12-25T20:46:31.072316Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID  CODE_GENDER  FLAG_OWN_CAR  FLAG_OWN_REALTY  CNT_CHILDREN  \\\n",
      "0  5008804            1             1                1           0.0   \n",
      "1  5008805            1             1                1           0.0   \n",
      "2  5008806            1             1                1           0.0   \n",
      "3  5008808            0             0                1           0.0   \n",
      "4  5008809            0             0                1           0.0   \n",
      "\n",
      "   AMT_INCOME_TOTAL  NAME_INCOME_TYPE  NAME_EDUCATION_TYPE  \\\n",
      "0          0.059697                 4                    1   \n",
      "1          0.059697                 4                    1   \n",
      "2          0.012850                 4                    4   \n",
      "3          0.036274                 0                    4   \n",
      "4          0.036274                 0                    4   \n",
      "\n",
      "   NAME_FAMILY_STATUS  NAME_HOUSING_TYPE  FLAG_WORK_PHONE  FLAG_PHONE  \\\n",
      "0                   0                  4              1.0         0.0   \n",
      "1                   0                  4              1.0         0.0   \n",
      "2                   1                  1              0.0         0.0   \n",
      "3                   3                  1              0.0         1.0   \n",
      "4                   3                  1              0.0         1.0   \n",
      "\n",
      "   FLAG_EMAIL  OCCUPATION_TYPE  CNT_FAM_MEMBERS       AGE  EMPLOYMENT_YEARS  \\\n",
      "0         0.0                8              2.0  0.254968          0.258576   \n",
      "1         0.0                8              2.0  0.254968          0.258576   \n",
      "2         0.0               16              2.0  0.789578          0.064045   \n",
      "3         1.0               14              1.0  0.656109          0.173469   \n",
      "4         1.0               14              1.0  0.656109          0.173469   \n",
      "\n",
      "   STATUS  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned_data.csv file\n",
    "data = pd.read_csv(\"data/cleaned_data.csv\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3890ec7695b5564e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T20:48:13.008703Z",
     "start_time": "2024-12-25T20:46:59.566123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features (indices): [7, 8, 9, 10, 11, 12, 14, 15, 16]\n",
      "Selected Features (names): ['NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'FLAG_WORK_PHONE', 'FLAG_PHONE', 'FLAG_EMAIL', 'CNT_FAM_MEMBERS', 'AGE', 'EMPLOYMENT_YEARS']\n",
      "Test Accuracy with Selected Features: 0.6430791735234961\n",
      "Selected features have been saved to 'selected_features_standard.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "#STANDARD GENETIC ALGORITHM\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data/cleaned_data.csv')\n",
    "\n",
    "# Assuming the last column is the target variable\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Parameters for Genetic Algorithm\n",
    "population_size = 20\n",
    "num_generations = 50\n",
    "mutation_rate = 0.1\n",
    "crossover_rate = 0.8\n",
    "\n",
    "# Fitness function: Validation accuracy of Decision Tree\n",
    "def fitness_function(chromosome):\n",
    "    selected_features = [bool(bit) for bit in chromosome]\n",
    "    X_train_selected = X_train.iloc[:, selected_features]\n",
    "    X_val_selected = X_val.iloc[:, selected_features]\n",
    "\n",
    "    if X_train_selected.shape[1] == 0:  # Avoid empty feature sets\n",
    "        return 0\n",
    "\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    predictions = model.predict(X_val_selected)\n",
    "    return accuracy_score(y_val, predictions)\n",
    "\n",
    "# Initialize population\n",
    "population = [np.random.randint(2, size=X.shape[1]).tolist() for _ in range(population_size)]\n",
    "\n",
    "# Genetic Algorithm main loop\n",
    "for generation in range(num_generations):\n",
    "    # Calculate fitness for each individual\n",
    "    fitness_scores = [fitness_function(chromosome) for chromosome in population]\n",
    "\n",
    "    # Select parents based on fitness (roulette wheel selection)\n",
    "    fitness_sum = sum(fitness_scores)\n",
    "    if fitness_sum == 0:\n",
    "        continue\n",
    "    probabilities = [score / fitness_sum for score in fitness_scores]\n",
    "    parents = random.choices(population, probabilities, k=population_size)\n",
    "\n",
    "    # Create next generation\n",
    "    next_generation = []\n",
    "    for i in range(0, population_size, 2):\n",
    "        parent1, parent2 = parents[i], parents[i + 1]\n",
    "\n",
    "        # Crossover\n",
    "        if random.random() < crossover_rate:\n",
    "            crossover_point = random.randint(1, len(parent1) - 1)\n",
    "            child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "            child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "        else:\n",
    "            child1, child2 = parent1, parent2\n",
    "\n",
    "        # Mutation\n",
    "        child1 = [bit if random.random() > mutation_rate else 1 - bit for bit in child1]\n",
    "        child2 = [bit if random.random() > mutation_rate else 1 - bit for bit in child2]\n",
    "\n",
    "        next_generation.extend([child1, child2])\n",
    "\n",
    "    # Elitism: Carry forward the best individual\n",
    "    best_individual = population[np.argmax(fitness_scores)]\n",
    "    next_generation[random.randint(0, population_size - 1)] = best_individual\n",
    "\n",
    "    population = next_generation\n",
    "\n",
    "# Get the best chromosome from the final population\n",
    "fitness_scores = [fitness_function(chromosome) for chromosome in population]\n",
    "best_chromosome = population[np.argmax(fitness_scores)]\n",
    "selected_features = [i for i, bit in enumerate(best_chromosome) if bit == 1]\n",
    "selected_features1 = selected_features\n",
    "\n",
    "print(\"Selected Features (indices):\", selected_features)\n",
    "print(\"Selected Features (names):\", X.columns[selected_features].tolist())\n",
    "\n",
    "# Evaluate the performance on the test set\n",
    "X_train_selected = X_train.iloc[:, selected_features]\n",
    "X_test_selected = X_test.iloc[:, selected_features]\n",
    "\n",
    "final_model = DecisionTreeClassifier()\n",
    "final_model.fit(X_train_selected, y_train)\n",
    "test_predictions = final_model.predict(X_test_selected)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Test Accuracy with Selected Features:\", test_accuracy)\n",
    "\n",
    "# Save selected features to a CSV file\n",
    "selected_features_df = pd.DataFrame({\n",
    "    'Selected_Features_Indices': selected_features,\n",
    "    'Selected_Features_Names': X.columns[selected_features]\n",
    "})\n",
    "\n",
    "selected_features_df.to_csv('selected_features_standard.csv', index=False)\n",
    "print(\"Selected features have been saved to 'selected_features_standard.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfeddcf4d029fc81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T20:49:53.176926Z",
     "start_time": "2024-12-25T20:48:32.509998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features (indices): [1, 2, 4, 6, 8, 9, 11, 13, 14, 15]\n",
      "Selected Features (names): ['CODE_GENDER', 'FLAG_OWN_CAR', 'CNT_CHILDREN', 'NAME_INCOME_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'FLAG_PHONE', 'OCCUPATION_TYPE', 'CNT_FAM_MEMBERS', 'AGE']\n",
      "Test Accuracy with Selected Features: 0.6460047540683854\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "#BINARY GENETIC ALGORITHM\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data/cleaned_data.csv')\n",
    "\n",
    "# Assuming the last column is the target variable\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Parameters for Genetic Algorithm\n",
    "population_size = 20\n",
    "num_generations = 50\n",
    "mutation_rate = 0.1\n",
    "crossover_rate = 0.8\n",
    "\n",
    "# Fitness function: Validation accuracy of Decision Tree\n",
    "def fitness_function(chromosome):\n",
    "    selected_features = [bool(bit) for bit in chromosome]\n",
    "    X_train_selected = X_train.iloc[:, selected_features]\n",
    "    X_val_selected = X_val.iloc[:, selected_features]\n",
    "\n",
    "    if X_train_selected.shape[1] == 0:  # Avoid empty feature sets\n",
    "        return 0\n",
    "\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    predictions = model.predict(X_val_selected)\n",
    "    return accuracy_score(y_val, predictions)\n",
    "\n",
    "# Initialize population\n",
    "population = [np.random.randint(2, size=X.shape[1]).tolist() for _ in range(population_size)]\n",
    "\n",
    "# Genetic Algorithm main loop\n",
    "for generation in range(num_generations):\n",
    "    # Calculate fitness for each individual\n",
    "    fitness_scores = [fitness_function(chromosome) for chromosome in population]\n",
    "\n",
    "    # Select parents based on fitness (roulette wheel selection)\n",
    "    fitness_sum = sum(fitness_scores)\n",
    "    if fitness_sum == 0:\n",
    "        continue\n",
    "    probabilities = [score / fitness_sum for score in fitness_scores]\n",
    "    parents = random.choices(population, probabilities, k=population_size)\n",
    "\n",
    "    # Create next generation\n",
    "    next_generation = []\n",
    "    for i in range(0, population_size, 2):\n",
    "        parent1, parent2 = parents[i], parents[i + 1]\n",
    "\n",
    "        # Crossover\n",
    "        if random.random() < crossover_rate:\n",
    "            crossover_point = random.randint(1, len(parent1) - 1)\n",
    "            child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "            child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "        else:\n",
    "            child1, child2 = parent1, parent2\n",
    "\n",
    "        # Mutation\n",
    "        child1 = [bit if random.random() > mutation_rate else 1 - bit for bit in child1]\n",
    "        child2 = [bit if random.random() > mutation_rate else 1 - bit for bit in child2]\n",
    "\n",
    "        next_generation.extend([child1, child2])\n",
    "\n",
    "    # Elitism: Carry forward the best individual\n",
    "    best_individual = population[np.argmax(fitness_scores)]\n",
    "    next_generation[random.randint(0, population_size - 1)] = best_individual\n",
    "\n",
    "    population = next_generation\n",
    "\n",
    "# Get the best chromosome from the final population\n",
    "fitness_scores = [fitness_function(chromosome) for chromosome in population]\n",
    "best_chromosome = population[np.argmax(fitness_scores)]\n",
    "selected_features = [i for i, bit in enumerate(best_chromosome) if bit == 1]\n",
    "\n",
    "print(\"Selected Features (indices):\", selected_features)\n",
    "print(\"Selected Features (names):\", X.columns[selected_features].tolist())\n",
    "\n",
    "# Evaluate the performance on the test set\n",
    "X_train_selected = X_train.iloc[:, selected_features]\n",
    "X_test_selected = X_test.iloc[:, selected_features]\n",
    "\n",
    "final_model = DecisionTreeClassifier()\n",
    "final_model.fit(X_train_selected, y_train)\n",
    "test_predictions = final_model.predict(X_test_selected)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Test Accuracy with Selected Features:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e12f1d3e3a5ecd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T20:51:06.312629Z",
     "start_time": "2024-12-25T20:49:57.816852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\n",
      "0  \t20    \n",
      "1  \t17    \n",
      "2  \t18    \n",
      "3  \t14    \n",
      "4  \t16    \n",
      "5  \t18    \n",
      "6  \t14    \n",
      "7  \t16    \n",
      "8  \t14    \n",
      "9  \t17    \n",
      "10 \t14    \n",
      "11 \t15    \n",
      "12 \t16    \n",
      "13 \t19    \n",
      "14 \t18    \n",
      "15 \t14    \n",
      "16 \t16    \n",
      "17 \t20    \n",
      "18 \t17    \n",
      "19 \t12    \n",
      "20 \t15    \n",
      "21 \t18    \n",
      "22 \t20    \n",
      "23 \t16    \n",
      "24 \t15    \n",
      "25 \t17    \n",
      "26 \t16    \n",
      "27 \t10    \n",
      "28 \t16    \n",
      "29 \t15    \n",
      "30 \t18    \n",
      "31 \t18    \n",
      "32 \t20    \n",
      "33 \t13    \n",
      "34 \t16    \n",
      "35 \t18    \n",
      "36 \t18    \n",
      "37 \t20    \n",
      "38 \t15    \n",
      "39 \t16    \n",
      "40 \t14    \n",
      "41 \t14    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Run the Genetic Algorithm\u001b[39;00m\n\u001b[0;32m     58\u001b[0m num_generations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m---> 59\u001b[0m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meaSimple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpopulation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoolbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcxpb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmutpb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mngen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_generations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhalloffame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     64\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Get the best individual\u001b[39;00m\n\u001b[0;32m     67\u001b[0m best_individual \u001b[38;5;241m=\u001b[39m tools\u001b[38;5;241m.\u001b[39mselBest(population, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\xampp\\htdocs\\Credit-Card-Approval-Prediction\\venv\\Lib\\site-packages\\deap\\algorithms.py:173\u001b[0m, in \u001b[0;36meaSimple\u001b[1;34m(population, toolbox, cxpb, mutpb, ngen, stats, halloffame, verbose)\u001b[0m\n\u001b[0;32m    171\u001b[0m invalid_ind \u001b[38;5;241m=\u001b[39m [ind \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m offspring \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ind\u001b[38;5;241m.\u001b[39mfitness\u001b[38;5;241m.\u001b[39mvalid]\n\u001b[0;32m    172\u001b[0m fitnesses \u001b[38;5;241m=\u001b[39m toolbox\u001b[38;5;241m.\u001b[39mmap(toolbox\u001b[38;5;241m.\u001b[39mevaluate, invalid_ind)\n\u001b[1;32m--> 173\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minvalid_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfitnesses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitness\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfit\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# Update the hall of fame with the generated individuals\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m, in \u001b[0;36mfitness_function\u001b[1;34m(individual)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Train a Decision Tree Classifier\u001b[39;00m\n\u001b[0;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Predict on the test set\u001b[39;00m\n\u001b[0;32m     31\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test\u001b[38;5;241m.\u001b[39miloc[:, selected_features])\n",
      "File \u001b[1;32mc:\\xampp\\htdocs\\Credit-Card-Approval-Prediction\\venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\xampp\\htdocs\\Credit-Card-Approval-Prediction\\venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1019\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    990\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \n\u001b[0;32m    992\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1019\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\xampp\\htdocs\\Credit-Card-Approval-Prediction\\venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data/cleaned_data.csv')\n",
    "\n",
    "# Assuming the last column is the target variable\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define the fitness function\n",
    "def fitness_function(individual):\n",
    "    selected_features = [index for index, bit in enumerate(individual) if bit == 1]\n",
    "    if len(selected_features) == 0:\n",
    "        return 0,  # Avoid empty feature sets\n",
    "\n",
    "    # Train a Decision Tree Classifier\n",
    "    model = DecisionTreeClassifier(random_state=42)\n",
    "    model.fit(X_train.iloc[:, selected_features], y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    predictions = model.predict(X_test.iloc[:, selected_features])\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return accuracy,\n",
    "\n",
    "# Genetic Algorithm Setup\n",
    "num_features = X_train.shape[1]\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Define the individual and population\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", np.random.randint, 2)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=num_features)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Register genetic operators\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.1)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "toolbox.register(\"evaluate\", fitness_function)\n",
    "\n",
    "# Initialize population\n",
    "population = toolbox.population(n=20)\n",
    "\n",
    "# Run the Genetic Algorithm\n",
    "num_generations = 50\n",
    "algorithms.eaSimple(\n",
    "    population, toolbox,\n",
    "    cxpb=0.8, mutpb=0.1,\n",
    "    ngen=num_generations, stats=None,\n",
    "    halloffame=None, verbose=True\n",
    ")\n",
    "\n",
    "# Get the best individual\n",
    "best_individual = tools.selBest(population, k=1)[0]\n",
    "selected_features = [index for index, bit in enumerate(best_individual) if bit == 1]\n",
    "\n",
    "print(f\"Selected feature indices: {selected_features}\")\n",
    "\n",
    "# Evaluate the final model using the selected features\n",
    "final_model = DecisionTreeClassifier(random_state=42)\n",
    "final_model.fit(X_train.iloc[:, selected_features], y_train)\n",
    "\n",
    "# Predict on the test set with the selected features\n",
    "test_predictions = final_model.predict(X_test.iloc[:, selected_features])\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "print(f\"Test Accuracy with selected features: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2093afae675d5379",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T20:52:11.769205Z",
     "start_time": "2024-12-25T20:51:16.220281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5400\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data/cleaned_data.csv')\n",
    "\n",
    "# Assuming the last column is the target variable\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "def train_test_split(X, y, test_size=0.3, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(X.shape[0] * (1 - test_size))\n",
    "    train_indices, test_indices = indices[:split], indices[split:]\n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Decision Tree Classifier Implementation\n",
    "class DecisionTreeClassifierScratch:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        num_samples, num_features = X.shape\n",
    "        num_labels = len(np.unique(y))\n",
    "\n",
    "        # Stopping criteria\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or num_labels == 1 or num_samples == 0:\n",
    "            return self._most_common_label(y)\n",
    "\n",
    "        # Find the best split\n",
    "        best_feature, best_threshold = self._best_split(X, y, num_features)\n",
    "        if best_feature is None:\n",
    "            return self._most_common_label(y)\n",
    "\n",
    "        # Split the data\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return {\n",
    "            'feature': best_feature,\n",
    "            'threshold': best_threshold,\n",
    "            'left': left_subtree,\n",
    "            'right': right_subtree\n",
    "        }\n",
    "\n",
    "    def _best_split(self, X, y, num_features):\n",
    "        best_gini = float('inf')\n",
    "        best_feature, best_threshold = None, None\n",
    "\n",
    "        for feature in range(num_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                gini = self._gini_impurity(X[:, feature], y, threshold)\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _gini_impurity(self, feature_column, y, threshold):\n",
    "        left_indices = feature_column <= threshold\n",
    "        right_indices = feature_column > threshold\n",
    "\n",
    "        n = len(y)\n",
    "        n_left, n_right = np.sum(left_indices), np.sum(right_indices)\n",
    "        if n_left == 0 or n_right == 0:\n",
    "            return 1.0\n",
    "\n",
    "        gini_left = 1.0 - sum((np.sum(y[left_indices] == c) / n_left) ** 2 for c in np.unique(y))\n",
    "        gini_right = 1.0 - sum((np.sum(y[right_indices] == c) / n_right) ** 2 for c in np.unique(y))\n",
    "\n",
    "        return (n_left / n) * gini_left + (n_right / n) * gini_right\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        if len(y) == 0:\n",
    "            return None\n",
    "        return np.bincount(y).argmax()\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, tree):\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "\n",
    "        if x[tree['feature']] <= tree['threshold']:\n",
    "            return self._traverse_tree(x, tree['left'])\n",
    "        else:\n",
    "            return self._traverse_tree(x, tree['right'])\n",
    "\n",
    "# Train and evaluate the decision tree\n",
    "clf = DecisionTreeClassifierScratch(max_depth=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b983226d8a0ce765",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T22:44:48.016437Z",
     "start_time": "2024-12-25T20:52:27.143593Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 128\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Genetic Algorithm main loop\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_generations):\n\u001b[1;32m--> 128\u001b[0m     fitness_scores \u001b[38;5;241m=\u001b[39m [\u001b[43mfitness_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchromosome\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m chromosome \u001b[38;5;129;01min\u001b[39;00m population]\n\u001b[0;32m    130\u001b[0m     fitness_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(fitness_scores)\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fitness_sum \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[1], line 121\u001b[0m, in \u001b[0;36mfitness_function\u001b[1;34m(chromosome)\u001b[0m\n\u001b[0;32m    119\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train_selected, y_train)\n\u001b[0;32m    120\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val_selected)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maccuracy_score\u001b[49m(y_val\u001b[38;5;241m.\u001b[39mvalues, predictions)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data/cleaned_data.csv')\n",
    "\n",
    "# Assuming the last column is the target variable\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "def train_test_split(X, y, test_size=0.3, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(X.shape[0] * (1 - test_size))\n",
    "    train_indices, test_indices = indices[:split], indices[split:]\n",
    "    return X.iloc[train_indices], X.iloc[test_indices], y.iloc[train_indices], y.iloc[test_indices]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Decision Tree Classifier Implementation\n",
    "class DecisionTreeClassifierScratch:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X.values, y.values)\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        num_samples, num_features = X.shape\n",
    "        num_labels = len(np.unique(y))\n",
    "\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or num_labels == 1 or num_samples == 0:\n",
    "            return self._most_common_label(y)\n",
    "\n",
    "        best_feature, best_threshold = self._best_split(X, y, num_features)\n",
    "        if best_feature is None:\n",
    "            return self._most_common_label(y)\n",
    "\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "\n",
    "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return {\n",
    "            'feature': best_feature,\n",
    "            'threshold': best_threshold,\n",
    "            'left': left_subtree,\n",
    "            'right': right_subtree\n",
    "        }\n",
    "\n",
    "    def _best_split(self, X, y, num_features):\n",
    "        best_gini = float('inf')\n",
    "        best_feature, best_threshold = None, None\n",
    "\n",
    "        for feature in range(num_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                gini = self._gini_impurity(X[:, feature], y, threshold)\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _gini_impurity(self, feature_column, y, threshold):\n",
    "        left_indices = feature_column <= threshold\n",
    "        right_indices = feature_column > threshold\n",
    "\n",
    "        n = len(y)\n",
    "        n_left, n_right = np.sum(left_indices), np.sum(right_indices)\n",
    "        if n_left == 0 or n_right == 0:\n",
    "            return 1.0\n",
    "\n",
    "        gini_left = 1.0 - sum((np.sum(y[left_indices] == c) / n_left) ** 2 for c in np.unique(y))\n",
    "        gini_right = 1.0 - sum((np.sum(y[right_indices] == c) / n_right) ** 2 for c in np.unique(y))\n",
    "\n",
    "        return (n_left / n) * gini_left + (n_right / n) * gini_right\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        if len(y) == 0:\n",
    "            return None\n",
    "        return np.bincount(y).argmax()\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X.values])\n",
    "\n",
    "    def _traverse_tree(self, x, tree):\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "\n",
    "        if x[tree['feature']] <= tree['threshold']:\n",
    "            return self._traverse_tree(x, tree['left'])\n",
    "        else:\n",
    "            return self._traverse_tree(x, tree['right'])\n",
    "\n",
    "# Genetic Algorithm for Feature Selection\n",
    "population_size = 20\n",
    "num_generations = 50\n",
    "mutation_rate = 0.1\n",
    "crossover_rate = 0.8\n",
    "\n",
    "# Fitness function: Validation accuracy of Decision Tree\n",
    "def fitness_function(chromosome):\n",
    "    selected_features = [bool(bit) for bit in chromosome]\n",
    "    X_train_selected = X_train.iloc[:, selected_features]\n",
    "    X_val_selected = X_val.iloc[:, selected_features]\n",
    "\n",
    "    if X_train_selected.shape[1] == 0:\n",
    "        return 0\n",
    "\n",
    "    model = DecisionTreeClassifierScratch(max_depth=3)\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    predictions = model.predict(X_val_selected)\n",
    "    return accuracy_score(y_val.values, predictions)\n",
    "\n",
    "# Initialize population\n",
    "population = [np.random.randint(2, size=X.shape[1]).tolist() for _ in range(population_size)]\n",
    "\n",
    "# Genetic Algorithm main loop\n",
    "for generation in range(num_generations):\n",
    "    fitness_scores = [fitness_function(chromosome) for chromosome in population]\n",
    "\n",
    "    fitness_sum = sum(fitness_scores)\n",
    "    if fitness_sum == 0:\n",
    "        continue\n",
    "    probabilities = [score / fitness_sum for score in fitness_scores]\n",
    "    parents = random.choices(population, probabilities, k=population_size)\n",
    "\n",
    "    next_generation = []\n",
    "    for i in range(0, population_size, 2):\n",
    "        parent1, parent2 = parents[i], parents[i + 1]\n",
    "\n",
    "        if random.random() < crossover_rate:\n",
    "            crossover_point = random.randint(1, len(parent1) - 1)\n",
    "            child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "            child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "        else:\n",
    "            child1, child2 = parent1, parent2\n",
    "\n",
    "        child1 = [bit if random.random() > mutation_rate else 1 - bit for bit in child1]\n",
    "        child2 = [bit if random.random() > mutation_rate else 1 - bit for bit in child2]\n",
    "\n",
    "        next_generation.extend([child1, child2])\n",
    "\n",
    "    best_individual = population[np.argmax(fitness_scores)]\n",
    "    next_generation[random.randint(0, population_size - 1)] = best_individual\n",
    "\n",
    "    population = next_generation\n",
    "\n",
    "fitness_scores = [fitness_function(chromosome) for chromosome in population]\n",
    "best_chromosome = population[np.argmax(fitness_scores)]\n",
    "selected_features = [i for i, bit in enumerate(best_chromosome) if bit == 1]\n",
    "\n",
    "print(\"Selected Features (indices):\", selected_features)\n",
    "print(\"Selected Features (names):\", X.columns[selected_features].tolist())\n",
    "\n",
    "# Evaluate on test set\n",
    "X_train_selected = X_train.iloc[:, selected_features]\n",
    "X_test_selected = X_test.iloc[:, selected_features]\n",
    "\n",
    "final_model = DecisionTreeClassifierScratch(max_depth=3)\n",
    "final_model.fit(X_train_selected, y_train)\n",
    "y_pred_test = final_model.predict(X_test_selected)\n",
    "\n",
    "accuracy = np.sum(y_test.values == y_pred_test) / len(y_test)\n",
    "print(f\"Test Accuracy with Selected Features: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27d745ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with Selected Features (KNN): 0.6266227829584933\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train and evaluate the KNN model with the selected features\n",
    "X_train_selected = X_train.iloc[:, selected_features]\n",
    "X_test_selected = X_test.iloc[:, selected_features]\n",
    "\n",
    "# Initialize KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3)  # You can adjust the number of neighbors\n",
    "\n",
    "# Train the model\n",
    "knn_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions\n",
    "test_predictions_knn = knn_model.predict(X_test_selected)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy_knn = accuracy_score(y_test, test_predictions_knn)\n",
    "print(\"Test Accuracy with Selected Features (KNN):\", test_accuracy_knn)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b3a8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with Selected Features (MLP): 0.5351983909307003\n"
     ]
    }
   ],
   "source": [
    "#mlp model\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Train and evaluate the MLP model with the selected features\n",
    "X_train_selected = X_train.iloc[:, selected_features]\n",
    "X_test_selected = X_test.iloc[:, selected_features]\n",
    "\n",
    "# Initialize MLP model\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(800,), max_iter=3000, random_state=42)  # You can adjust the parameters\n",
    "\n",
    "# Train the model\n",
    "mlp_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions\n",
    "test_predictions_mlp = mlp_model.predict(X_test_selected)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy_mlp = accuracy_score(y_test, test_predictions_mlp)\n",
    "print(\"Test Accuracy with Selected Features (MLP):\", test_accuracy_mlp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
