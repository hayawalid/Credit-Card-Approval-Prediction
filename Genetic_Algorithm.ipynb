{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-25T20:46:31.139212Z",
     "start_time": "2024-12-25T20:46:31.072316Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned_data.csv file\n",
    "data = pd.read_csv(\"data/cleaned_data.csv\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(data.head())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID  CODE_GENDER  FLAG_OWN_CAR  FLAG_OWN_REALTY  CNT_CHILDREN  \\\n",
      "0  5008804            1             1                1           0.0   \n",
      "1  5008805            1             1                1           0.0   \n",
      "2  5008806            1             1                1           0.0   \n",
      "3  5008808            0             0                1           0.0   \n",
      "4  5008809            0             0                1           0.0   \n",
      "\n",
      "   AMT_INCOME_TOTAL  NAME_INCOME_TYPE  NAME_EDUCATION_TYPE  \\\n",
      "0          0.258721                 4                    1   \n",
      "1          0.258721                 4                    1   \n",
      "2          0.055233                 4                    4   \n",
      "3          0.156977                 0                    4   \n",
      "4          0.156977                 0                    4   \n",
      "\n",
      "   NAME_FAMILY_STATUS  NAME_HOUSING_TYPE  FLAG_WORK_PHONE  FLAG_PHONE  \\\n",
      "0                   0                  4              1.0         0.0   \n",
      "1                   0                  4              1.0         0.0   \n",
      "2                   1                  1              0.0         0.0   \n",
      "3                   3                  1              0.0         1.0   \n",
      "4                   3                  1              0.0         1.0   \n",
      "\n",
      "   FLAG_EMAIL  OCCUPATION_TYPE  CNT_FAM_MEMBERS       AGE  EMPLOYMENT_YEARS  \\\n",
      "0         0.0                8         0.052632  0.254968          0.258576   \n",
      "1         0.0                8         0.052632  0.254968          0.258576   \n",
      "2         0.0               16         0.052632  0.789578          0.064045   \n",
      "3         1.0               14         0.000000  0.656109          0.173469   \n",
      "4         1.0               14         0.000000  0.656109          0.173469   \n",
      "\n",
      "   STATUS  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T20:48:13.008703Z",
     "start_time": "2024-12-25T20:46:59.566123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "#STANDARD GENETIC ALGORITHM\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data/cleaned_data.csv')\n",
    "\n",
    "# Assuming the last column is the target variable\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Parameters for Genetic Algorithm\n",
    "population_size = 20\n",
    "num_generations = 50\n",
    "mutation_rate = 0.1\n",
    "crossover_rate = 0.8\n",
    "\n",
    "# Fitness function: Validation accuracy of Decision Tree\n",
    "def fitness_function(chromosome):\n",
    "    selected_features = [bool(bit) for bit in chromosome]\n",
    "    X_train_selected = X_train.iloc[:, selected_features]\n",
    "    X_val_selected = X_val.iloc[:, selected_features]\n",
    "\n",
    "    if X_train_selected.shape[1] == 0:  # Avoid empty feature sets\n",
    "        return 0\n",
    "\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    predictions = model.predict(X_val_selected)\n",
    "    return accuracy_score(y_val, predictions)\n",
    "\n",
    "# Initialize population\n",
    "population = [np.random.randint(2, size=X.shape[1]).tolist() for _ in range(population_size)]\n",
    "\n",
    "# Genetic Algorithm main loop\n",
    "for generation in range(num_generations):\n",
    "    # Calculate fitness for each individual\n",
    "    fitness_scores = [fitness_function(chromosome) for chromosome in population]\n",
    "\n",
    "    # Select parents based on fitness (roulette wheel selection)\n",
    "    fitness_sum = sum(fitness_scores)\n",
    "    if fitness_sum == 0:\n",
    "        continue\n",
    "    probabilities = [score / fitness_sum for score in fitness_scores]\n",
    "    parents = random.choices(population, probabilities, k=population_size)\n",
    "\n",
    "    # Create next generation\n",
    "    next_generation = []\n",
    "    for i in range(0, population_size, 2):\n",
    "        parent1, parent2 = parents[i], parents[i + 1]\n",
    "\n",
    "        # Crossover\n",
    "        if random.random() < crossover_rate:\n",
    "            crossover_point = random.randint(1, len(parent1) - 1)\n",
    "            child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "            child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "        else:\n",
    "            child1, child2 = parent1, parent2\n",
    "\n",
    "        # Mutation\n",
    "        child1 = [bit if random.random() > mutation_rate else 1 - bit for bit in child1]\n",
    "        child2 = [bit if random.random() > mutation_rate else 1 - bit for bit in child2]\n",
    "\n",
    "        next_generation.extend([child1, child2])\n",
    "\n",
    "    # Elitism: Carry forward the best individual\n",
    "    best_individual = population[np.argmax(fitness_scores)]\n",
    "    next_generation[random.randint(0, population_size - 1)] = best_individual\n",
    "\n",
    "    population = next_generation\n",
    "\n",
    "# Get the best chromosome from the final population\n",
    "fitness_scores = [fitness_function(chromosome) for chromosome in population]\n",
    "best_chromosome = population[np.argmax(fitness_scores)]\n",
    "selected_features = [i for i, bit in enumerate(best_chromosome) if bit == 1]\n",
    "\n",
    "print(\"Selected Features (indices):\", selected_features)\n",
    "print(\"Selected Features (names):\", X.columns[selected_features].tolist())\n",
    "\n",
    "# Evaluate the performance on the test set\n",
    "X_train_selected = X_train.iloc[:, selected_features]\n",
    "X_test_selected = X_test.iloc[:, selected_features]\n",
    "\n",
    "final_model = DecisionTreeClassifier()\n",
    "final_model.fit(X_train_selected, y_train)\n",
    "test_predictions = final_model.predict(X_test_selected)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Test Accuracy with Selected Features:\", test_accuracy)\n"
   ],
   "id": "3890ec7695b5564e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features (indices): [1, 4, 6, 7, 10, 11, 12, 13, 14, 15, 16]\n",
      "Selected Features (names): ['CODE_GENDER', 'CNT_CHILDREN', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'FLAG_WORK_PHONE', 'FLAG_PHONE', 'FLAG_EMAIL', 'OCCUPATION_TYPE', 'CNT_FAM_MEMBERS', 'AGE', 'EMPLOYMENT_YEARS']\n",
      "Test Accuracy with Selected Features: 0.6452733589321631\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T20:49:53.176926Z",
     "start_time": "2024-12-25T20:48:32.509998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "#BINARY GENETIC ALGORITHM\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data/cleaned_data.csv')\n",
    "\n",
    "# Assuming the last column is the target variable\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Parameters for Genetic Algorithm\n",
    "population_size = 20\n",
    "num_generations = 50\n",
    "mutation_rate = 0.1\n",
    "crossover_rate = 0.8\n",
    "\n",
    "# Fitness function: Validation accuracy of Decision Tree\n",
    "def fitness_function(chromosome):\n",
    "    selected_features = [bool(bit) for bit in chromosome]\n",
    "    X_train_selected = X_train.iloc[:, selected_features]\n",
    "    X_val_selected = X_val.iloc[:, selected_features]\n",
    "\n",
    "    if X_train_selected.shape[1] == 0:  # Avoid empty feature sets\n",
    "        return 0\n",
    "\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    predictions = model.predict(X_val_selected)\n",
    "    return accuracy_score(y_val, predictions)\n",
    "\n",
    "# Initialize population\n",
    "population = [np.random.randint(2, size=X.shape[1]).tolist() for _ in range(population_size)]\n",
    "\n",
    "# Genetic Algorithm main loop\n",
    "for generation in range(num_generations):\n",
    "    # Calculate fitness for each individual\n",
    "    fitness_scores = [fitness_function(chromosome) for chromosome in population]\n",
    "\n",
    "    # Select parents based on fitness (roulette wheel selection)\n",
    "    fitness_sum = sum(fitness_scores)\n",
    "    if fitness_sum == 0:\n",
    "        continue\n",
    "    probabilities = [score / fitness_sum for score in fitness_scores]\n",
    "    parents = random.choices(population, probabilities, k=population_size)\n",
    "\n",
    "    # Create next generation\n",
    "    next_generation = []\n",
    "    for i in range(0, population_size, 2):\n",
    "        parent1, parent2 = parents[i], parents[i + 1]\n",
    "\n",
    "        # Crossover\n",
    "        if random.random() < crossover_rate:\n",
    "            crossover_point = random.randint(1, len(parent1) - 1)\n",
    "            child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "            child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "        else:\n",
    "            child1, child2 = parent1, parent2\n",
    "\n",
    "        # Mutation\n",
    "        child1 = [bit if random.random() > mutation_rate else 1 - bit for bit in child1]\n",
    "        child2 = [bit if random.random() > mutation_rate else 1 - bit for bit in child2]\n",
    "\n",
    "        next_generation.extend([child1, child2])\n",
    "\n",
    "    # Elitism: Carry forward the best individual\n",
    "    best_individual = population[np.argmax(fitness_scores)]\n",
    "    next_generation[random.randint(0, population_size - 1)] = best_individual\n",
    "\n",
    "    population = next_generation\n",
    "\n",
    "# Get the best chromosome from the final population\n",
    "fitness_scores = [fitness_function(chromosome) for chromosome in population]\n",
    "best_chromosome = population[np.argmax(fitness_scores)]\n",
    "selected_features = [i for i, bit in enumerate(best_chromosome) if bit == 1]\n",
    "\n",
    "print(\"Selected Features (indices):\", selected_features)\n",
    "print(\"Selected Features (names):\", X.columns[selected_features].tolist())\n",
    "\n",
    "# Evaluate the performance on the test set\n",
    "X_train_selected = X_train.iloc[:, selected_features]\n",
    "X_test_selected = X_test.iloc[:, selected_features]\n",
    "\n",
    "final_model = DecisionTreeClassifier()\n",
    "final_model.fit(X_train_selected, y_train)\n",
    "test_predictions = final_model.predict(X_test_selected)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Test Accuracy with Selected Features:\", test_accuracy)\n"
   ],
   "id": "dfeddcf4d029fc81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features (indices): [1, 4, 6, 8, 10, 11, 12, 13, 14, 15, 16]\n",
      "Selected Features (names): ['CODE_GENDER', 'CNT_CHILDREN', 'NAME_INCOME_TYPE', 'NAME_FAMILY_STATUS', 'FLAG_WORK_PHONE', 'FLAG_PHONE', 'FLAG_EMAIL', 'OCCUPATION_TYPE', 'CNT_FAM_MEMBERS', 'AGE', 'EMPLOYMENT_YEARS']\n",
      "Test Accuracy with Selected Features: 0.6452733589321631\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T20:51:06.312629Z",
     "start_time": "2024-12-25T20:49:57.816852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data/cleaned_data.csv')\n",
    "\n",
    "# Assuming the last column is the target variable\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define the fitness function\n",
    "def fitness_function(individual):\n",
    "    selected_features = [index for index, bit in enumerate(individual) if bit == 1]\n",
    "    if len(selected_features) == 0:\n",
    "        return 0,  # Avoid empty feature sets\n",
    "\n",
    "    # Train a Decision Tree Classifier\n",
    "    model = DecisionTreeClassifier(random_state=42)\n",
    "    model.fit(X_train.iloc[:, selected_features], y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    predictions = model.predict(X_test.iloc[:, selected_features])\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return accuracy,\n",
    "\n",
    "# Genetic Algorithm Setup\n",
    "num_features = X_train.shape[1]\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Define the individual and population\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", np.random.randint, 2)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=num_features)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Register genetic operators\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.1)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "toolbox.register(\"evaluate\", fitness_function)\n",
    "\n",
    "# Initialize population\n",
    "population = toolbox.population(n=20)\n",
    "\n",
    "# Run the Genetic Algorithm\n",
    "num_generations = 50\n",
    "algorithms.eaSimple(\n",
    "    population, toolbox,\n",
    "    cxpb=0.8, mutpb=0.1,\n",
    "    ngen=num_generations, stats=None,\n",
    "    halloffame=None, verbose=True\n",
    ")\n",
    "\n",
    "# Get the best individual\n",
    "best_individual = tools.selBest(population, k=1)[0]\n",
    "selected_features = [index for index, bit in enumerate(best_individual) if bit == 1]\n",
    "\n",
    "print(f\"Selected feature indices: {selected_features}\")\n",
    "\n",
    "# Evaluate the final model using the selected features\n",
    "final_model = DecisionTreeClassifier(random_state=42)\n",
    "final_model.fit(X_train.iloc[:, selected_features], y_train)\n",
    "\n",
    "# Predict on the test set with the selected features\n",
    "test_predictions = final_model.predict(X_test.iloc[:, selected_features])\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "print(f\"Test Accuracy with selected features: {test_accuracy}\")\n"
   ],
   "id": "4e12f1d3e3a5ecd1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\n",
      "0  \t20    \n",
      "1  \t17    \n",
      "2  \t20    \n",
      "3  \t15    \n",
      "4  \t19    \n",
      "5  \t9     \n",
      "6  \t16    \n",
      "7  \t18    \n",
      "8  \t20    \n",
      "9  \t20    \n",
      "10 \t16    \n",
      "11 \t19    \n",
      "12 \t20    \n",
      "13 \t19    \n",
      "14 \t16    \n",
      "15 \t19    \n",
      "16 \t12    \n",
      "17 \t17    \n",
      "18 \t16    \n",
      "19 \t14    \n",
      "20 \t16    \n",
      "21 \t16    \n",
      "22 \t16    \n",
      "23 \t17    \n",
      "24 \t15    \n",
      "25 \t18    \n",
      "26 \t18    \n",
      "27 \t20    \n",
      "28 \t11    \n",
      "29 \t20    \n",
      "30 \t20    \n",
      "31 \t17    \n",
      "32 \t14    \n",
      "33 \t20    \n",
      "34 \t14    \n",
      "35 \t10    \n",
      "36 \t17    \n",
      "37 \t11    \n",
      "38 \t16    \n",
      "39 \t12    \n",
      "40 \t20    \n",
      "41 \t16    \n",
      "42 \t17    \n",
      "43 \t17    \n",
      "44 \t16    \n",
      "45 \t16    \n",
      "46 \t19    \n",
      "47 \t14    \n",
      "48 \t15    \n",
      "49 \t16    \n",
      "50 \t17    \n",
      "Selected feature indices: [1, 4, 5, 6, 7, 10, 12, 15, 16]\n",
      "Test Accuracy with selected features: 0.651947339550192\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T20:52:11.769205Z",
     "start_time": "2024-12-25T20:51:16.220281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data/cleaned_data.csv')\n",
    "\n",
    "# Assuming the last column is the target variable\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "def train_test_split(X, y, test_size=0.3, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(X.shape[0] * (1 - test_size))\n",
    "    train_indices, test_indices = indices[:split], indices[split:]\n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Decision Tree Classifier Implementation\n",
    "class DecisionTreeClassifierScratch:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        num_samples, num_features = X.shape\n",
    "        num_labels = len(np.unique(y))\n",
    "\n",
    "        # Stopping criteria\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or num_labels == 1 or num_samples == 0:\n",
    "            return self._most_common_label(y)\n",
    "\n",
    "        # Find the best split\n",
    "        best_feature, best_threshold = self._best_split(X, y, num_features)\n",
    "        if best_feature is None:\n",
    "            return self._most_common_label(y)\n",
    "\n",
    "        # Split the data\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return {\n",
    "            'feature': best_feature,\n",
    "            'threshold': best_threshold,\n",
    "            'left': left_subtree,\n",
    "            'right': right_subtree\n",
    "        }\n",
    "\n",
    "    def _best_split(self, X, y, num_features):\n",
    "        best_gini = float('inf')\n",
    "        best_feature, best_threshold = None, None\n",
    "\n",
    "        for feature in range(num_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                gini = self._gini_impurity(X[:, feature], y, threshold)\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _gini_impurity(self, feature_column, y, threshold):\n",
    "        left_indices = feature_column <= threshold\n",
    "        right_indices = feature_column > threshold\n",
    "\n",
    "        n = len(y)\n",
    "        n_left, n_right = np.sum(left_indices), np.sum(right_indices)\n",
    "        if n_left == 0 or n_right == 0:\n",
    "            return 1.0\n",
    "\n",
    "        gini_left = 1.0 - sum((np.sum(y[left_indices] == c) / n_left) ** 2 for c in np.unique(y))\n",
    "        gini_right = 1.0 - sum((np.sum(y[right_indices] == c) / n_right) ** 2 for c in np.unique(y))\n",
    "\n",
    "        return (n_left / n) * gini_left + (n_right / n) * gini_right\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        if len(y) == 0:\n",
    "            return None\n",
    "        return np.bincount(y).argmax()\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, tree):\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "\n",
    "        if x[tree['feature']] <= tree['threshold']:\n",
    "            return self._traverse_tree(x, tree['left'])\n",
    "        else:\n",
    "            return self._traverse_tree(x, tree['right'])\n",
    "\n",
    "# Train and evaluate the decision tree\n",
    "clf = DecisionTreeClassifierScratch(max_depth=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ],
   "id": "2093afae675d5379",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5400\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T22:44:48.016437Z",
     "start_time": "2024-12-25T20:52:27.143593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data/cleaned_data.csv')\n",
    "\n",
    "# Assuming the last column is the target variable\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "def train_test_split(X, y, test_size=0.3, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(X.shape[0] * (1 - test_size))\n",
    "    train_indices, test_indices = indices[:split], indices[split:]\n",
    "    return X.iloc[train_indices], X.iloc[test_indices], y.iloc[train_indices], y.iloc[test_indices]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Decision Tree Classifier Implementation\n",
    "class DecisionTreeClassifierScratch:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X.values, y.values)\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        num_samples, num_features = X.shape\n",
    "        num_labels = len(np.unique(y))\n",
    "\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or num_labels == 1 or num_samples == 0:\n",
    "            return self._most_common_label(y)\n",
    "\n",
    "        best_feature, best_threshold = self._best_split(X, y, num_features)\n",
    "        if best_feature is None:\n",
    "            return self._most_common_label(y)\n",
    "\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "\n",
    "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return {\n",
    "            'feature': best_feature,\n",
    "            'threshold': best_threshold,\n",
    "            'left': left_subtree,\n",
    "            'right': right_subtree\n",
    "        }\n",
    "\n",
    "    def _best_split(self, X, y, num_features):\n",
    "        best_gini = float('inf')\n",
    "        best_feature, best_threshold = None, None\n",
    "\n",
    "        for feature in range(num_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                gini = self._gini_impurity(X[:, feature], y, threshold)\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _gini_impurity(self, feature_column, y, threshold):\n",
    "        left_indices = feature_column <= threshold\n",
    "        right_indices = feature_column > threshold\n",
    "\n",
    "        n = len(y)\n",
    "        n_left, n_right = np.sum(left_indices), np.sum(right_indices)\n",
    "        if n_left == 0 or n_right == 0:\n",
    "            return 1.0\n",
    "\n",
    "        gini_left = 1.0 - sum((np.sum(y[left_indices] == c) / n_left) ** 2 for c in np.unique(y))\n",
    "        gini_right = 1.0 - sum((np.sum(y[right_indices] == c) / n_right) ** 2 for c in np.unique(y))\n",
    "\n",
    "        return (n_left / n) * gini_left + (n_right / n) * gini_right\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        if len(y) == 0:\n",
    "            return None\n",
    "        return np.bincount(y).argmax()\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X.values])\n",
    "\n",
    "    def _traverse_tree(self, x, tree):\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "\n",
    "        if x[tree['feature']] <= tree['threshold']:\n",
    "            return self._traverse_tree(x, tree['left'])\n",
    "        else:\n",
    "            return self._traverse_tree(x, tree['right'])\n",
    "\n",
    "# Genetic Algorithm for Feature Selection\n",
    "population_size = 20\n",
    "num_generations = 50\n",
    "mutation_rate = 0.1\n",
    "crossover_rate = 0.8\n",
    "\n",
    "# Fitness function: Validation accuracy of Decision Tree\n",
    "def fitness_function(chromosome):\n",
    "    selected_features = [bool(bit) for bit in chromosome]\n",
    "    X_train_selected = X_train.iloc[:, selected_features]\n",
    "    X_val_selected = X_val.iloc[:, selected_features]\n",
    "\n",
    "    if X_train_selected.shape[1] == 0:\n",
    "        return 0\n",
    "\n",
    "    model = DecisionTreeClassifierScratch(max_depth=3)\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    predictions = model.predict(X_val_selected)\n",
    "    return accuracy_score(y_val.values, predictions)\n",
    "\n",
    "# Initialize population\n",
    "population = [np.random.randint(2, size=X.shape[1]).tolist() for _ in range(population_size)]\n",
    "\n",
    "# Genetic Algorithm main loop\n",
    "for generation in range(num_generations):\n",
    "    fitness_scores = [fitness_function(chromosome) for chromosome in population]\n",
    "\n",
    "    fitness_sum = sum(fitness_scores)\n",
    "    if fitness_sum == 0:\n",
    "        continue\n",
    "    probabilities = [score / fitness_sum for score in fitness_scores]\n",
    "    parents = random.choices(population, probabilities, k=population_size)\n",
    "\n",
    "    next_generation = []\n",
    "    for i in range(0, population_size, 2):\n",
    "        parent1, parent2 = parents[i], parents[i + 1]\n",
    "\n",
    "        if random.random() < crossover_rate:\n",
    "            crossover_point = random.randint(1, len(parent1) - 1)\n",
    "            child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "            child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "        else:\n",
    "            child1, child2 = parent1, parent2\n",
    "\n",
    "        child1 = [bit if random.random() > mutation_rate else 1 - bit for bit in child1]\n",
    "        child2 = [bit if random.random() > mutation_rate else 1 - bit for bit in child2]\n",
    "\n",
    "        next_generation.extend([child1, child2])\n",
    "\n",
    "    best_individual = population[np.argmax(fitness_scores)]\n",
    "    next_generation[random.randint(0, population_size - 1)] = best_individual\n",
    "\n",
    "    population = next_generation\n",
    "\n",
    "fitness_scores = [fitness_function(chromosome) for chromosome in population]\n",
    "best_chromosome = population[np.argmax(fitness_scores)]\n",
    "selected_features = [i for i, bit in enumerate(best_chromosome) if bit == 1]\n",
    "\n",
    "print(\"Selected Features (indices):\", selected_features)\n",
    "print(\"Selected Features (names):\", X.columns[selected_features].tolist())\n",
    "\n",
    "# Evaluate on test set\n",
    "X_train_selected = X_train.iloc[:, selected_features]\n",
    "X_test_selected = X_test.iloc[:, selected_features]\n",
    "\n",
    "final_model = DecisionTreeClassifierScratch(max_depth=3)\n",
    "final_model.fit(X_train_selected, y_train)\n",
    "y_pred_test = final_model.predict(X_test_selected)\n",
    "\n",
    "accuracy = np.sum(y_test.values == y_pred_test) / len(y_test)\n",
    "print(f\"Test Accuracy with Selected Features: {accuracy:.4f}\")\n"
   ],
   "id": "b983226d8a0ce765",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 128\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[38;5;66;03m# Genetic Algorithm main loop\u001B[39;00m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m generation \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_generations):\n\u001B[0;32m--> 128\u001B[0m     fitness_scores \u001B[38;5;241m=\u001B[39m [\u001B[43mfitness_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchromosome\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m chromosome \u001B[38;5;129;01min\u001B[39;00m population]\n\u001B[1;32m    130\u001B[0m     fitness_sum \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(fitness_scores)\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m fitness_sum \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "Cell \u001B[0;32mIn[9], line 119\u001B[0m, in \u001B[0;36mfitness_function\u001B[0;34m(chromosome)\u001B[0m\n\u001B[1;32m    116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    118\u001B[0m model \u001B[38;5;241m=\u001B[39m DecisionTreeClassifierScratch(max_depth\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m--> 119\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_selected\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    120\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_val_selected)\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m accuracy_score(y_val\u001B[38;5;241m.\u001B[39mvalues, predictions)\n",
      "Cell \u001B[0;32mIn[9], line 31\u001B[0m, in \u001B[0;36mDecisionTreeClassifierScratch.fit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y):\n\u001B[0;32m---> 31\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtree \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_build_tree\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[9], line 48\u001B[0m, in \u001B[0;36mDecisionTreeClassifierScratch._build_tree\u001B[0;34m(self, X, y, depth)\u001B[0m\n\u001B[1;32m     45\u001B[0m right_indices \u001B[38;5;241m=\u001B[39m X[:, best_feature] \u001B[38;5;241m>\u001B[39m best_threshold\n\u001B[1;32m     47\u001B[0m left_subtree \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_tree(X[left_indices], y[left_indices], depth \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 48\u001B[0m right_subtree \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_build_tree\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[43mright_indices\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m[\u001B[49m\u001B[43mright_indices\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdepth\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeature\u001B[39m\u001B[38;5;124m'\u001B[39m: best_feature,\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mthreshold\u001B[39m\u001B[38;5;124m'\u001B[39m: best_threshold,\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m'\u001B[39m: left_subtree,\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mright\u001B[39m\u001B[38;5;124m'\u001B[39m: right_subtree\n\u001B[1;32m     55\u001B[0m }\n",
      "Cell \u001B[0;32mIn[9], line 40\u001B[0m, in \u001B[0;36mDecisionTreeClassifierScratch._build_tree\u001B[0;34m(self, X, y, depth)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_depth \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m depth \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_depth) \u001B[38;5;129;01mor\u001B[39;00m num_labels \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m num_samples \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_most_common_label(y)\n\u001B[0;32m---> 40\u001B[0m best_feature, best_threshold \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_best_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m best_feature \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_most_common_label(y)\n",
      "Cell \u001B[0;32mIn[9], line 64\u001B[0m, in \u001B[0;36mDecisionTreeClassifierScratch._best_split\u001B[0;34m(self, X, y, num_features)\u001B[0m\n\u001B[1;32m     62\u001B[0m thresholds \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(X[:, feature])\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m threshold \u001B[38;5;129;01min\u001B[39;00m thresholds:\n\u001B[0;32m---> 64\u001B[0m     gini \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gini_impurity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthreshold\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gini \u001B[38;5;241m<\u001B[39m best_gini:\n\u001B[1;32m     66\u001B[0m         best_gini \u001B[38;5;241m=\u001B[39m gini\n",
      "Cell \u001B[0;32mIn[9], line 81\u001B[0m, in \u001B[0;36mDecisionTreeClassifierScratch._gini_impurity\u001B[0;34m(self, feature_column, y, threshold)\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_left \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m n_right \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m1.0\u001B[39m\n\u001B[0;32m---> 81\u001B[0m gini_left \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28;43msum\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m[\u001B[49m\u001B[43mleft_indices\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mn_left\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munique\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     82\u001B[0m gini_right \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28msum\u001B[39m((np\u001B[38;5;241m.\u001B[39msum(y[right_indices] \u001B[38;5;241m==\u001B[39m c) \u001B[38;5;241m/\u001B[39m n_right) \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m np\u001B[38;5;241m.\u001B[39munique(y))\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (n_left \u001B[38;5;241m/\u001B[39m n) \u001B[38;5;241m*\u001B[39m gini_left \u001B[38;5;241m+\u001B[39m (n_right \u001B[38;5;241m/\u001B[39m n) \u001B[38;5;241m*\u001B[39m gini_right\n",
      "Cell \u001B[0;32mIn[9], line 81\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_left \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m n_right \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m1.0\u001B[39m\n\u001B[0;32m---> 81\u001B[0m gini_left \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28msum\u001B[39m((\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m[\u001B[49m\u001B[43mleft_indices\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m/\u001B[39m n_left) \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m np\u001B[38;5;241m.\u001B[39munique(y))\n\u001B[1;32m     82\u001B[0m gini_right \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28msum\u001B[39m((np\u001B[38;5;241m.\u001B[39msum(y[right_indices] \u001B[38;5;241m==\u001B[39m c) \u001B[38;5;241m/\u001B[39m n_right) \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m np\u001B[38;5;241m.\u001B[39munique(y))\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (n_left \u001B[38;5;241m/\u001B[39m n) \u001B[38;5;241m*\u001B[39m gini_left \u001B[38;5;241m+\u001B[39m (n_right \u001B[38;5;241m/\u001B[39m n) \u001B[38;5;241m*\u001B[39m gini_right\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/numpy/core/fromnumeric.py:2172\u001B[0m, in \u001B[0;36m_sum_dispatcher\u001B[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001B[0m\n\u001B[1;32m   2102\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2103\u001B[0m \u001B[38;5;124;03m    Clip (limit) the values in an array.\u001B[39;00m\n\u001B[1;32m   2104\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2167\u001B[0m \n\u001B[1;32m   2168\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   2169\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapfunc(a, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclip\u001B[39m\u001B[38;5;124m'\u001B[39m, a_min, a_max, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m-> 2172\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_sum_dispatcher\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   2173\u001B[0m                     initial\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, where\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m   2174\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (a, out)\n\u001B[1;32m   2177\u001B[0m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_sum_dispatcher)\n\u001B[1;32m   2178\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msum\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, keepdims\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39m_NoValue,\n\u001B[1;32m   2179\u001B[0m         initial\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39m_NoValue, where\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39m_NoValue):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
